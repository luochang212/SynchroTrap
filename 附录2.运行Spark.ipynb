{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c562b68-c9b4-4505-bd57-3298cec82634",
   "metadata": {},
   "source": [
    "# 附录2：运行 Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4110dd10-4b0e-45f6-b4a5-499399a49788",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T11:56:06.446946Z",
     "iopub.status.busy": "2024-07-30T11:56:06.446488Z",
     "iopub.status.idle": "2024-07-30T11:56:06.451486Z",
     "shell.execute_reply": "2024-07-30T11:56:06.450580Z",
     "shell.execute_reply.started": "2024-07-30T11:56:06.446911Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ce0ca1-f0ac-4678-a3eb-ddd41252bc0d",
   "metadata": {},
   "source": [
    "## 1. 检查 Spark 环境\n",
    "\n",
    "运行一段简单的代码，检查 Spark 环境是否可用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1dd4d59-05dd-4b93-8717-6e9e78143c6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T11:56:06.453198Z",
     "iopub.status.busy": "2024-07-30T11:56:06.452688Z",
     "iopub.status.idle": "2024-07-30T11:56:18.535490Z",
     "shell.execute_reply": "2024-07-30T11:56:18.534475Z",
     "shell.execute_reply.started": "2024-07-30T11:56:06.453177Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/30 19:56:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|name|\n",
      "+---+----+\n",
      "|  1|   A|\n",
      "|  2|   B|\n",
      "|  3|   C|\n",
      "|  4|   D|\n",
      "|  5|   E|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 创建 SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"App\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "vertices = spark.createDataFrame([\n",
    "    (\"1\", \"A\"),\n",
    "    (\"2\", \"B\"),\n",
    "    (\"3\", \"C\"),\n",
    "    (\"4\", \"D\"),\n",
    "    (\"5\", \"E\")\n",
    "], [\"id\", \"name\"])\n",
    "\n",
    "vertices.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4085061-b008-4ab9-bce7-54487a5acc7f",
   "metadata": {},
   "source": [
    "## 2. 安装 & 使用 graphframes\n",
    "\n",
    "[graphframes](https://github.com/graphframes/graphframes) 是 Spark 的一个图计算库。\n",
    "\n",
    "首先安装 graphframes:\n",
    "\n",
    "```bash\n",
    "pip install graphframes\n",
    "```\n",
    "\n",
    "由于 graphframes 需要 jar 文件配合，无法直接在 Jupyter 中运行。请移步 [./graph/task.sh](./graph/task.sh)。\n",
    "\n",
    "在 `task.sh` 中，我们用 `spark-submit` 执行一个简单的图连通算法。运行以下代码：\n",
    "\n",
    "```bash\n",
    "cd graph\n",
    "sh task.sh\n",
    "```\n",
    "\n",
    "预期的计算结果如下：\n",
    "\n",
    "```\n",
    "+---+-------+---+-------------+\n",
    "| id|   name|age|    component|\n",
    "+---+-------+---+-------------+\n",
    "|  g|  Gabby| 60| 146028888064|\n",
    "|  f|  Fanny| 36| 412316860416|\n",
    "|  e| Esther| 32| 670014898176|\n",
    "|  d|  David| 29| 670014898176|\n",
    "|  c|Charlie| 30|1047972020224|\n",
    "|  b|    Bob| 36|1047972020224|\n",
    "|  a|  Alice| 34| 670014898176|\n",
    "+---+-------+---+-------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacc24e4-e9df-4ae9-b07c-ac755cf8a9d4",
   "metadata": {},
   "source": [
    "## 3. 定制 Python 环境\n",
    "\n",
    "1）将名为 `myenv` 的本地环境，打包成 `myenv.tar.gz` 压缩文件。\n",
    "\n",
    "```bash\n",
    "conda activate myenv\n",
    "conda install conda-pack\n",
    "conda pack -o myenv.tar.gz\n",
    "```\n",
    "\n",
    "2）运行以下代码，将 `myenv.tar.gz` 文件上传到 Spark 节点。\n",
    "\n",
    "```bash\n",
    "# arguments\n",
    "APP_NAME=\"SPARK_TASK\"\n",
    "P1D=$(date -d \"1 day ago\" +%Y-%m-%d)\n",
    "QUEUE_NAME=\"bigdata\"\n",
    "files=\"FILE_A,FILE_B\"\n",
    "py_path=\"PATH/TO/YOUR_PYTHON_FILE.py\"\n",
    "table_name=\"YOUR_DB.YOUR_TABLE\"\n",
    "\n",
    "# task\n",
    "spark-submit \\\n",
    "--master yarn \\\n",
    "--deploy-mode cluster \\\n",
    "--name \"${APP_NAME}_${P1D}\" \\\n",
    "--queue \"${QUEUE_NAME}\" \\\n",
    "--priority HIGH \\\n",
    "--conf spark.executor.memory=4g \\\n",
    "--conf spark.dynamicAllocation.minExecutors=800 \\\n",
    "--conf spark.pyspark.driver.python=./PythonEnv/bin/python \\\n",
    "--conf spark.pyspark.python=./PythonEnv/bin/python \\\n",
    "--files \"${files}\" \\\n",
    "--archives hdfs/path/to/myenv.tar.gz#PythonEnv \\\n",
    "  \"${py_path}\" \\\n",
    "    --date \"${P1D}\" \\\n",
    "    --outputTable \"${table_name}\"\n",
    "\n",
    "# info\n",
    "if [ $? -ne 0 ]; then\n",
    "    echo -e \"[INFO] ${APP_NAME} failed.\"\n",
    "else\n",
    "    echo -e \"[INFO] ${APP_NAME} done.\"\n",
    "fi\n",
    "```\n",
    "\n",
    "> **Note:** 使用 `--archives [HDFS_PATH]#[FOLDER_NAME]` 参数，Spark 会将 `[HDFS_PATH]` 路径上的压缩包文件解压到名为 `[FOLDER_NAME]` 的文件夹中。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a666cd3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
